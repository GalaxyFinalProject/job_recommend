{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í¬ë¡¤ë§í•œ ì´ˆê¸° ë°ì´í„° "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ê° í¬ë¡¤ë§í•œ csv ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df1=pd.read_csv(r'C:\\Users\\Playdata\\Desktop\\programers_cp949_1.csv', encoding='cp949')\n",
    "df2=pd.read_csv(r\"C:\\Users\\Playdata\\Desktop\\wanted_cp949_1.csv\", encoding='cp949')\n",
    "df3=pd.read_csv(r\"C:\\Users\\Playdata\\Desktop\\jumpit_cp949_1.csv\", encoding='cp949')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì§ë¬´ ê¸°ìˆ  ìŠ¤íƒ ë¬¶ì–´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ë¬´&ê¸°ìˆ ìŠ¤íƒ ë³´ê¸°\n",
    "def add_positions_from_csv(file_path, combined_position_stack):\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # í—¤ë” ê±´ë„ˆë›°ê¸°\n",
    "        for row in reader:\n",
    "            position = row[0]\n",
    "            stack_item = row[1]\n",
    "            if position in combined_position_stack:\n",
    "                combined_position_stack[position].add(stack_item)\n",
    "            else:\n",
    "                combined_position_stack[position] = {stack_item}\n",
    "    return combined_position_stack\n",
    "\n",
    "combined_position_stack = {}  \n",
    "\n",
    "# ì„¸ ê°œì˜ CSV íŒŒì¼ í•©ì¹˜ê¸°\n",
    "combined_position_stack = add_positions_from_csv(r\"C:\\Users\\Playdata\\Desktop\\position_stack1.csv\", combined_position_stack)\n",
    "combined_position_stack = add_positions_from_csv(r\"C:\\Users\\Playdata\\Desktop\\position_stack2.csv\", combined_position_stack)\n",
    "combined_position_stack = add_positions_from_csv(r\"C:\\Users\\Playdata\\Desktop\\position_stack3.csv\", combined_position_stack)\n",
    "\n",
    "#ìµœì¢… ì €ì¥\n",
    "directory = r'C:\\Users\\Playdata\\Desktop\\\\' \n",
    "\n",
    "filename1 = os.path.join(directory, 'combined_position_stack.csv')\n",
    "\n",
    "with open(filename1, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Position', 'Stack'])  # í—¤ë” ì‘ì„±\n",
    "    for position, stack in combined_position_stack.items():\n",
    "        for stack_item in stack:\n",
    "            writer.writerow([position, stack_item])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê°™ì€ ê³µê³  ì²˜ë¦¬"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ & ì›í‹°ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì‚´ì§ ì „ì²˜ë¦¬\n",
    "def preprocess_dataframe(df):\n",
    "    df['ê³µê³ ëª…'] = df['ê³µê³ ëª…'].str.replace('ë²¡ì—”ë“œ', 'ë°±ì—”ë“œ')\n",
    "    df['íšŒì‚¬ëª…'] = df['íšŒì‚¬ëª…'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "    return df\n",
    "\n",
    "df1 = preprocess_dataframe(df1)\n",
    "df2 = preprocess_dataframe(df2)\n",
    "df3 = preprocess_dataframe(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ê³µê³ ì— ëª¨ë‘ ìˆëŠ” íšŒì‚¬ëª… ë°ì´í„° ë¹¼ë‚´ê¸° filtered_df\n",
    "def preprocess_and_compare_links(df,intersection):\n",
    "    # ê³µí†µìœ¼ë¡œ ìˆëŠ” íšŒì‚¬ë§Œ í•„í„°ë§\n",
    "    filtered_df = df[df['íšŒì‚¬ëª…'].isin(intersection)].copy() # Avoid SettingWithCopyWarning by explicitly creating a copy\n",
    "    # NaNì¸ ê¸°ìˆ ìŠ¤íƒì„ '[]'ë¡œ ëŒ€ì²´\n",
    "    filtered_df.loc[filtered_df['ê¸°ìˆ ìŠ¤íƒ'].apply(lambda x: isinstance(x, float)), 'ê¸°ìˆ ìŠ¤íƒ'] = filtered_df.loc[filtered_df['ê¸°ìˆ ìŠ¤íƒ'].apply(lambda x: isinstance(x, float)), 'ê¸°ìˆ ìŠ¤íƒ'].apply(lambda x: '[]' if pd.isnull(x) else x)\n",
    "    # ê³µê³ ëª…ì—ì„œ 'ì‹ ì…'ê³¼ ()ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶€ë¶„ ì‚­ì œ\n",
    "    filtered_df.loc[:, 'ê³µê³ ëª…'] = filtered_df['ê³µê³ ëª…'].apply(lambda x: re.sub(r' \\(ì‹ ì….*?\\)|ì‹ ì…|\\[ì‹ ì….*?\\] |\\[ì½”ìŠ¤ë‹¥ìƒì¥ì‚¬\\] |ì±„ìš©', '', x))\n",
    "    filtered_df.loc[:, 'ê³µê³ ëª…'] = filtered_df['ê³µê³ ëª…'].apply(lambda x: re.sub(r'\\(ì‹ ì…/ê²½ë ¥\\)', '', x))\n",
    "\n",
    "    # ì¤‘ë³µë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ ì¶”ì¶œ\n",
    "    df = df[~df['ë§í¬'].isin(filtered_df['ë§í¬'])]\n",
    "    \n",
    "    return df,filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df1,df2])\n",
    "intersection = pd.Series(list(set(df1['íšŒì‚¬ëª…'].unique()) & set(df2['íšŒì‚¬ëª…'].unique())))\n",
    "df, filtered_df = preprocess_and_compare_links(df, intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íšŒì‚¬ê°€ ê°™ê³ , ë§í¬ëŠ” ë‹¤ë¥´ê³ , í•œ ê³µê³ ë¬¸ì´ ë‹¤ë¥¸ ê³µê³ ë¬¸ì„ ì™„ì „íˆ í¬í•¨í•˜ê³  ìˆì„ ë•Œ \n",
    "# ì¤‘ë³µëœ ê³µê³ ëª…ì„ ê°€ì§„ ê·¸ë£¹ ì°¾ê¸°\n",
    "def find_exact_matches(grouped, url_a, url_b):\n",
    "    exact_match_df = pd.DataFrame(columns=grouped.first().columns)\n",
    "    for _, group in grouped:\n",
    "        if len(group) > 1:\n",
    "            for i in range(len(group)):\n",
    "                a_group = group.iloc[i].copy() # copy() method to avoid SettingWithCopyWarning\n",
    "                for j in range(i + 1, len(group)):\n",
    "                    b_group = group.iloc[j].copy()\n",
    "                    if ((a_group['ë§í¬'].startswith(url_a) and b_group['ë§í¬'].startswith(url_b))\n",
    "                        or (a_group['ë§í¬'].startswith(url_b) and b_group['ë§í¬'].startswith(url_a))):\n",
    "                        if a_group['ê³µê³ ëª…'].replace(\" \", \"\") == b_group['ê³µê³ ëª…'].replace(\" \", \"\"):\n",
    "                            if a_group['ê¸°ìˆ ìŠ¤íƒ'] != b_group['ê¸°ìˆ ìŠ¤íƒ']:\n",
    "                                combined_tech_stack = set()\n",
    "\n",
    "                                if isinstance(a_group['ê¸°ìˆ ìŠ¤íƒ'], str):\n",
    "                                    a_group['ê¸°ìˆ ìŠ¤íƒ'] = ast.literal_eval(a_group['ê¸°ìˆ ìŠ¤íƒ'])\n",
    "                                    for a_stack in a_group['ê¸°ìˆ ìŠ¤íƒ']:\n",
    "                                        combined_tech_stack.add(a_stack)\n",
    "\n",
    "                                if isinstance(b_group['ê¸°ìˆ ìŠ¤íƒ'], str):\n",
    "                                    b_group['ê¸°ìˆ ìŠ¤íƒ'] = ast.literal_eval(b_group['ê¸°ìˆ ìŠ¤íƒ'])\n",
    "                                    for b_stack in b_group['ê¸°ìˆ ìŠ¤íƒ']:\n",
    "                                        combined_tech_stack.add(b_stack)\n",
    "                                    \n",
    "                                combined_tech_stack = list(combined_tech_stack) \n",
    "\n",
    "                                a_group['ê¸°ìˆ ìŠ¤íƒ'] = combined_tech_stack\n",
    "                                b_group['ê¸°ìˆ ìŠ¤íƒ'] = combined_tech_stack\n",
    "\n",
    "                            # Use pd.concat instead of DataFrame.append\n",
    "                            exact_match_df = pd.concat([exact_match_df, pd.DataFrame(a_group).T], ignore_index=True)\n",
    "                            exact_match_df = pd.concat([exact_match_df, pd.DataFrame(b_group).T], ignore_index=True)\n",
    "    return exact_match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í¬í•¨ê´€ê³„ì¸ ê³µê³ ë¬¸ ì°¾ê¸°\n",
    "def find_included_matches(grouped, url_a, url_b):\n",
    "    included_df = pd.DataFrame(columns=grouped.first().columns)\n",
    "    for _, group in grouped:\n",
    "        if len(group) > 1:\n",
    "            for i in range(len(group)):\n",
    "                a_group = group.iloc[i].copy() # copy() method to avoid SettingWithCopyWarning\n",
    "                for j in range(i + 1, len(group)):\n",
    "                    b_group = group.iloc[j].copy()\n",
    "                    if ((a_group['ë§í¬'].startswith(url_a) and b_group['ë§í¬'].startswith(url_b))\n",
    "                        or (a_group['ë§í¬'].startswith(url_b) and b_group['ë§í¬'].startswith(url_a))):\n",
    "                        if (a_group['ê³µê³ ëª…'].replace(\" \", \"\") in b_group['ê³µê³ ëª…'].replace(\" \", \"\") \n",
    "                        or b_group['ê³µê³ ëª…'].replace(\" \", \"\") in a_group['ê³µê³ ëª…'].replace(\" \", \"\")):\n",
    "                            if a_group['ê¸°ìˆ ìŠ¤íƒ'] != b_group['ê¸°ìˆ ìŠ¤íƒ']:\n",
    "                                combined_tech_stack = set()\n",
    "\n",
    "                                if isinstance(a_group['ê¸°ìˆ ìŠ¤íƒ'], str):\n",
    "                                    a_group['ê¸°ìˆ ìŠ¤íƒ'] = ast.literal_eval(a_group['ê¸°ìˆ ìŠ¤íƒ'])\n",
    "                                    for a_stack in a_group['ê¸°ìˆ ìŠ¤íƒ']:\n",
    "                                        combined_tech_stack.add(a_stack)\n",
    "\n",
    "                                if isinstance(b_group['ê¸°ìˆ ìŠ¤íƒ'], str):\n",
    "                                    b_group['ê¸°ìˆ ìŠ¤íƒ'] = ast.literal_eval(b_group['ê¸°ìˆ ìŠ¤íƒ'])\n",
    "                                    for b_stack in b_group['ê¸°ìˆ ìŠ¤íƒ']:\n",
    "                                        combined_tech_stack.add(b_stack)\n",
    "\n",
    "                                combined_tech_stack = list(combined_tech_stack) \n",
    "\n",
    "                                a_group['ê¸°ìˆ ìŠ¤íƒ'] = combined_tech_stack\n",
    "                                b_group['ê¸°ìˆ ìŠ¤íƒ'] = combined_tech_stack\n",
    "\n",
    "                            # Use pd.concat instead of DataFrame.append\n",
    "                            included_df = pd.concat([included_df, pd.DataFrame(a_group).T], ignore_index=True)\n",
    "                            included_df = pd.concat([included_df, pd.DataFrame(b_group).T], ignore_index=True)\n",
    "    return included_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(filtered_df, exact_match_df, included_df, link_start):\n",
    "    solo_df = filtered_df[~filtered_df['ë§í¬'].isin(exact_match_df['ë§í¬'])]\n",
    "    solo_df = solo_df[~solo_df['ë§í¬'].isin(included_df['ë§í¬'])]\n",
    "\n",
    "    exact_match_df['ê¸°ìˆ ìŠ¤íƒ'] = exact_match_df['ê¸°ìˆ ìŠ¤íƒ'].apply(lambda x: str(x))\n",
    "    included_df['ê¸°ìˆ ìŠ¤íƒ'] = included_df['ê¸°ìˆ ìŠ¤íƒ'].apply(lambda x: str(x))\n",
    "\n",
    "    duplicates_df = pd.merge(exact_match_df, included_df, how='outer')\n",
    "\n",
    "    duplicates_df = duplicates_df[duplicates_df['ë§í¬'].str.startswith(link_start)]\n",
    "\n",
    "    # solo_dfë¥¼ ë”í•¨\n",
    "    final_df = pd.concat([df, solo_df], ignore_index=True)\n",
    "\n",
    "    # duplicates_dfë¥¼ ë”í•¨\n",
    "    final_df = pd.concat([final_df, duplicates_df], ignore_index=True)\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df.groupby('íšŒì‚¬ëª…')\n",
    "exact_match_df = find_exact_matches(grouped, 'https://programmers.co.kr', 'https://www.wanted.co.kr')\n",
    "included_df = find_included_matches(grouped, 'https://programmers.co.kr', 'https://www.wanted.co.kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_wanted_df = process_df(filtered_df, exact_match_df, included_df, 'https://programmers.co.kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ê³µê³ ëª…</th>\n",
       "      <th>íšŒì‚¬ëª…</th>\n",
       "      <th>ì§ë¬´</th>\n",
       "      <th>ë§ˆê°ì¼</th>\n",
       "      <th>ê·¼ë¬´ì§€</th>\n",
       "      <th>ê¸°ìˆ ìŠ¤íƒ</th>\n",
       "      <th>ë§í¬</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ì €ì‘ê¶Œ ë³´í˜¸ ì„œë¹„ìŠ¤ ê°œë°œ</td>\n",
       "      <td>ë‘ë‹¤ì§€</td>\n",
       "      <td>ë°±ì—”ë“œ, ë°ì´í„° ì—”ì§€ë‹ˆì–´</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ì„œìš¸ ì„œì´ˆêµ¬ ë§¤í—Œë¡œ 16, (ì–‘ì¬ë™) 1206í˜¸ ë‘ë‹¤ì§€</td>\n",
       "      <td>['Docker', 'Python', 'Kubernetes']</td>\n",
       "      <td>https://programmers.co.kr/job_positions/18147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>ì´ëŸ¬ë‹ ì›¹ ì†”ë£¨ì…˜ ê°œë°œì</td>\n",
       "      <td>Xinics</td>\n",
       "      <td>ë°±ì—”ë“œ, í”„ë¡ íŠ¸ì—”ë“œ, ì›¹ê°œë°œ</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ êµ¬ë¡œêµ¬ ë””ì§€í„¸ë¡œ31ê¸¸ 53, 1101í˜¸(êµ¬ë¡œë™, ì´ì—”ì”¨ë²¤ì²˜ë“œë¦¼íƒ€ì›Œ5ì°¨)</td>\n",
       "      <td>['HTML', 'CSS', 'JavaScript', 'React', 'PHP']</td>\n",
       "      <td>https://programmers.co.kr/job_positions/1207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>ë°±ì—”ë“œ ì—”ì§€ë‹ˆì–´ (Python)</td>\n",
       "      <td>ë°ì´í„°ë¹„</td>\n",
       "      <td>ë°±ì—”ë“œ</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ì„œìš¸ ë§ˆí¬êµ¬ ë°±ë²”ë¡œ31ê¸¸ 21, (ê³µë•ë™) 404í˜¸</td>\n",
       "      <td>['Python', 'Redis', 'SQL']</td>\n",
       "      <td>https://programmers.co.kr/job_positions/17780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>ë°±ì—”ë“œ ì—”ì§€ë‹ˆì–´ (NodeJS)</td>\n",
       "      <td>ë°ì´í„°ë¹„</td>\n",
       "      <td>ë°±ì—”ë“œ</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ì„œìš¸ ë§ˆí¬êµ¬ ë°±ë²”ë¡œ31ê¸¸ 21, (ê³µë•ë™) 404í˜¸</td>\n",
       "      <td>['Node.js', 'Git']</td>\n",
       "      <td>https://programmers.co.kr/job_positions/17779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>C# ê°œë°œì</td>\n",
       "      <td>ë„¥ì„œìŠ¤ì»¤ë®¤ë‹ˆí‹°</td>\n",
       "      <td>ë°±ì—”ë“œ</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ì„œìš¸ ì˜ë“±í¬êµ¬ ì—¬ì˜ëŒ€ë¡œ 108, (ì—¬ì˜ë„ë™) íŒŒí¬ì› íƒ€ì›Œ2 19F</td>\n",
       "      <td>['C#', 'MariaDB']</td>\n",
       "      <td>https://programmers.co.kr/job_positions/4351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>30</td>\n",
       "      <td>í´ë¼ìš°ë“œEDI ì„œë¹„ìŠ¤ ê°œë°œì</td>\n",
       "      <td>ì¸ìŠ¤í”¼ì–¸</td>\n",
       "      <td>ë°±ì—”ë“œ</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ê¸ˆì²œêµ¬ ë²šê½ƒë¡œ 278, SJí…Œí¬ë…¸ë¹Œ 1309í˜¸</td>\n",
       "      <td>['Java', 'Spring']</td>\n",
       "      <td>https://programmers.co.kr/job_positions/13713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>212</td>\n",
       "      <td>Cloud ìš´ì˜ ê°œë°œ</td>\n",
       "      <td>ì§€ë‹ˆì–¸ìŠ¤</td>\n",
       "      <td>DevOps</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ê²½ê¸°ë„ ì•ˆì–‘ì‹œ ë™ì•ˆêµ¬ ë²Œë§ë¡œ 66 í‰ì´Œì—­ í•˜ì´í•„ë“œ ì§€ì‹ì‚°ì—…ì„¼í„° Aë™ 12ì¸µ</td>\n",
       "      <td>['MySQL', 'AWS', 'Docker', 'Kubernetes', 'Node...</td>\n",
       "      <td>https://programmers.co.kr/job_positions/17741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>63</td>\n",
       "      <td>Junior Backend Engineer</td>\n",
       "      <td>íŠ¸ë¦¿ì§€</td>\n",
       "      <td>ë°±ì—”ë“œ, ì›¹ê°œë°œ, ë°ì´í„° ì—”ì§€ë‹ˆì–´</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ë°©ë°°ë¡œ 226, 2, 4, 5, 6, ë³„ê´€2ì¸µ(ë°©ë°°ë™, ë„¥ì„¼ê°•ë‚¨íƒ€ì›Œ)</td>\n",
       "      <td>['MySQL', 'Docker', 'Django', 'Python', 'Node....</td>\n",
       "      <td>https://programmers.co.kr/job_positions/15444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>69</td>\n",
       "      <td>Junior Data Engineer</td>\n",
       "      <td>íŠ¸ë¦¿ì§€</td>\n",
       "      <td>ë°±ì—”ë“œ, ë°ì´í„° ì—”ì§€ë‹ˆì–´</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ë°©ë°°ë¡œ 226, 2, 4, 5, 6, ë³„ê´€2ì¸µ(ë°©ë°°ë™, ë„¥ì„¼ê°•ë‚¨íƒ€ì›Œ)</td>\n",
       "      <td>['Node.js', 'Django', 'Kafka', 'Hadoop', 'Pyth...</td>\n",
       "      <td>https://programmers.co.kr/job_positions/14660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>70</td>\n",
       "      <td>Data Acquisition Engineer</td>\n",
       "      <td>íŠ¸ë¦¿ì§€</td>\n",
       "      <td>ë°±ì—”ë“œ, ë°ì´í„° ì—”ì§€ë‹ˆì–´</td>\n",
       "      <td>ìƒì‹œ ì±„ìš©</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ë°©ë°°ë¡œ 226, 2, 4, 5, 6, ë³„ê´€2ì¸µ(ë°©ë°°ë™, ë„¥ì„¼ê°•ë‚¨íƒ€ì›Œ)</td>\n",
       "      <td>['Docker', 'Django', 'Python', 'Node.js', 'SQL']</td>\n",
       "      <td>https://programmers.co.kr/job_positions/14657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>823 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                        ê³µê³ ëª…      íšŒì‚¬ëª…                  ì§ë¬´    ë§ˆê°ì¼   \n",
       "0            0              ì €ì‘ê¶Œ ë³´í˜¸ ì„œë¹„ìŠ¤ ê°œë°œ      ë‘ë‹¤ì§€       ë°±ì—”ë“œ, ë°ì´í„° ì—”ì§€ë‹ˆì–´  ìƒì‹œ ì±„ìš©  \\\n",
       "1            4              ì´ëŸ¬ë‹ ì›¹ ì†”ë£¨ì…˜ ê°œë°œì   Xinics     ë°±ì—”ë“œ, í”„ë¡ íŠ¸ì—”ë“œ, ì›¹ê°œë°œ  ìƒì‹œ ì±„ìš©   \n",
       "2            5          ë°±ì—”ë“œ ì—”ì§€ë‹ˆì–´ (Python)     ë°ì´í„°ë¹„                 ë°±ì—”ë“œ  ìƒì‹œ ì±„ìš©   \n",
       "3            6          ë°±ì—”ë“œ ì—”ì§€ë‹ˆì–´ (NodeJS)     ë°ì´í„°ë¹„                 ë°±ì—”ë“œ  ìƒì‹œ ì±„ìš©   \n",
       "4            7                     C# ê°œë°œì  ë„¥ì„œìŠ¤ì»¤ë®¤ë‹ˆí‹°                 ë°±ì—”ë“œ  ìƒì‹œ ì±„ìš©   \n",
       "..         ...                        ...      ...                 ...    ...   \n",
       "818         30            í´ë¼ìš°ë“œEDI ì„œë¹„ìŠ¤ ê°œë°œì     ì¸ìŠ¤í”¼ì–¸                 ë°±ì—”ë“œ  ìƒì‹œ ì±„ìš©   \n",
       "819        212                Cloud ìš´ì˜ ê°œë°œ     ì§€ë‹ˆì–¸ìŠ¤              DevOps  ìƒì‹œ ì±„ìš©   \n",
       "820         63   Junior Backend Engineer       íŠ¸ë¦¿ì§€  ë°±ì—”ë“œ, ì›¹ê°œë°œ, ë°ì´í„° ì—”ì§€ë‹ˆì–´  ìƒì‹œ ì±„ìš©   \n",
       "821         69       Junior Data Engineer      íŠ¸ë¦¿ì§€       ë°±ì—”ë“œ, ë°ì´í„° ì—”ì§€ë‹ˆì–´  ìƒì‹œ ì±„ìš©   \n",
       "822         70  Data Acquisition Engineer      íŠ¸ë¦¿ì§€       ë°±ì—”ë“œ, ë°ì´í„° ì—”ì§€ë‹ˆì–´  ìƒì‹œ ì±„ìš©   \n",
       "\n",
       "                                                  ê·¼ë¬´ì§€   \n",
       "0                      ì„œìš¸ ì„œì´ˆêµ¬ ë§¤í—Œë¡œ 16, (ì–‘ì¬ë™) 1206í˜¸ ë‘ë‹¤ì§€  \\\n",
       "1       ì„œìš¸íŠ¹ë³„ì‹œ êµ¬ë¡œêµ¬ ë””ì§€í„¸ë¡œ31ê¸¸ 53, 1101í˜¸(êµ¬ë¡œë™, ì´ì—”ì”¨ë²¤ì²˜ë“œë¦¼íƒ€ì›Œ5ì°¨)   \n",
       "2                        ì„œìš¸ ë§ˆí¬êµ¬ ë°±ë²”ë¡œ31ê¸¸ 21, (ê³µë•ë™) 404í˜¸   \n",
       "3                        ì„œìš¸ ë§ˆí¬êµ¬ ë°±ë²”ë¡œ31ê¸¸ 21, (ê³µë•ë™) 404í˜¸   \n",
       "4                ì„œìš¸ ì˜ë“±í¬êµ¬ ì—¬ì˜ëŒ€ë¡œ 108, (ì—¬ì˜ë„ë™) íŒŒí¬ì› íƒ€ì›Œ2 19F   \n",
       "..                                                ...   \n",
       "818                   ì„œìš¸íŠ¹ë³„ì‹œ ê¸ˆì²œêµ¬ ë²šê½ƒë¡œ 278, SJí…Œí¬ë…¸ë¹Œ 1309í˜¸   \n",
       "819         ê²½ê¸°ë„ ì•ˆì–‘ì‹œ ë™ì•ˆêµ¬ ë²Œë§ë¡œ 66 í‰ì´Œì—­ í•˜ì´í•„ë“œ ì§€ì‹ì‚°ì—…ì„¼í„° Aë™ 12ì¸µ   \n",
       "820  ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ë°©ë°°ë¡œ 226, 2, 4, 5, 6, ë³„ê´€2ì¸µ(ë°©ë°°ë™, ë„¥ì„¼ê°•ë‚¨íƒ€ì›Œ)   \n",
       "821  ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ë°©ë°°ë¡œ 226, 2, 4, 5, 6, ë³„ê´€2ì¸µ(ë°©ë°°ë™, ë„¥ì„¼ê°•ë‚¨íƒ€ì›Œ)   \n",
       "822  ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ë°©ë°°ë¡œ 226, 2, 4, 5, 6, ë³„ê´€2ì¸µ(ë°©ë°°ë™, ë„¥ì„¼ê°•ë‚¨íƒ€ì›Œ)   \n",
       "\n",
       "                                                  ê¸°ìˆ ìŠ¤íƒ   \n",
       "0                   ['Docker', 'Python', 'Kubernetes']  \\\n",
       "1        ['HTML', 'CSS', 'JavaScript', 'React', 'PHP']   \n",
       "2                           ['Python', 'Redis', 'SQL']   \n",
       "3                                   ['Node.js', 'Git']   \n",
       "4                                    ['C#', 'MariaDB']   \n",
       "..                                                 ...   \n",
       "818                                 ['Java', 'Spring']   \n",
       "819  ['MySQL', 'AWS', 'Docker', 'Kubernetes', 'Node...   \n",
       "820  ['MySQL', 'Docker', 'Django', 'Python', 'Node....   \n",
       "821  ['Node.js', 'Django', 'Kafka', 'Hadoop', 'Pyth...   \n",
       "822   ['Docker', 'Django', 'Python', 'Node.js', 'SQL']   \n",
       "\n",
       "                                                ë§í¬  \n",
       "0    https://programmers.co.kr/job_positions/18147  \n",
       "1     https://programmers.co.kr/job_positions/1207  \n",
       "2    https://programmers.co.kr/job_positions/17780  \n",
       "3    https://programmers.co.kr/job_positions/17779  \n",
       "4     https://programmers.co.kr/job_positions/4351  \n",
       "..                                             ...  \n",
       "818  https://programmers.co.kr/job_positions/13713  \n",
       "819  https://programmers.co.kr/job_positions/17741  \n",
       "820  https://programmers.co.kr/job_positions/15444  \n",
       "821  https://programmers.co.kr/job_positions/14660  \n",
       "822  https://programmers.co.kr/job_positions/14657  \n",
       "\n",
       "[823 rows x 8 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_wanted_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì í•ê¹Œì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df í•©ì¹˜ê¸°\n",
    "df = pd.concat([pro_wanted_df, df3])\n",
    "# df3ì˜ ê·¼ë¬´ì§€ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('\\nÂ·', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = pd.Series(list(set(pro_wanted_df['íšŒì‚¬ëª…'].unique()) & set(df3['íšŒì‚¬ëª…'].unique())))\n",
    "\n",
    "df,filtered_df=preprocess_and_compare_links(df,intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íšŒì‚¬ê°€ ê°™ê³ , ë§í¬ëŠ” ë‹¤ë¥´ê³ , í•œ ê³µê³ ë¬¸ì´ ë‹¤ë¥¸ ê³µê³ ë¬¸ì„ ì™„ì „íˆ í¬í•¨í•˜ê³  ìˆì„ ë•Œ \n",
    "# ì¤‘ë³µëœ ê³µê³ ëª…ì„ ê°€ì§„ ê·¸ë£¹ ì°¾ê¸°\n",
    "grouped = filtered_df.groupby('íšŒì‚¬ëª…')\n",
    "\n",
    "# ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë¥¼ ì°¾ëŠ” ì½”ë“œ\n",
    "exact_match_df = pd.DataFrame(columns=filtered_df.columns)\n",
    "exact_match_df = pd.concat([exact_match_df, find_exact_matches(grouped, 'https://programmers.co.kr', 'https://www.jumpit.co.kr')])\n",
    "exact_match_df = pd.concat([exact_match_df, find_exact_matches(grouped, 'https://www.wanted.co.kr', 'https://www.jumpit.co.kr')])\n",
    "\n",
    "# í¬í•¨ê´€ê³„ì¸ ê²½ìš°ë¥¼ ì°¾ëŠ” ì½”ë“œ\n",
    "included_df = pd.DataFrame(columns=filtered_df.columns)\n",
    "included_df = pd.concat([included_df, find_included_matches(grouped, 'https://programmers.co.kr', 'https://www.jumpit.co.kr')])\n",
    "included_df = pd.concat([included_df, find_included_matches(grouped, 'https://www.wanted.co.kr', 'https://www.jumpit.co.kr')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = process_df(filtered_df, exact_match_df, included_df, 'https://www.jumpit.co.kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_df.to_csv(r'C:\\Users\\Playdata\\Desktop\\final_df.csv', index=True, encoding='cp949')\n",
    "len(final_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì§ë¬´ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df\n",
    "df['ì§ë¬´'] = df['ì§ë¬´'].str.split(', ')\n",
    "\n",
    "# ì›¹ê°œë°œ í•˜ë‚˜ë§Œ ìˆëŠ” ê²½ìš°\n",
    "df['ì§ë¬´'] = df['ì§ë¬´'].apply(lambda x: [\"í”„ë¡ íŠ¸ì—”ë“œ\", \"ë°±ì—”ë“œ\"] if isinstance(x, list) and x == [\"ì›¹ê°œë°œ\"] else x)\n",
    "\n",
    "# ì›¹ê°œë°œì´ í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš° ì›¹ê°œë°œì„ ì‚­ì œ\n",
    "# df['ì§ë¬´'] = df['ì§ë¬´'].apply(lambda x: [job for job in x if job != \"ì›¹ê°œë°œ\"])\n",
    "df['ì§ë¬´'] = df['ì§ë¬´'].apply(lambda x: [job for job in x if job != \"ì›¹ê°œë°œ\"] if isinstance(x, list) else [])\n",
    "\n",
    "\n",
    "df.to_csv(r'C:\\Users\\Playdata\\Desktop\\df_pos.csv', index=False, encoding='cp949')\n",
    "df=pd.read_csv(r'C:\\Users\\Playdata\\Desktop\\df_pos.csv', encoding='cp949')\n",
    "\n",
    "df['ì§ë¬´'] = df['ì§ë¬´'].str.replace(\"'\", '\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ìˆ ìŠ¤íƒ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(condition, x, y)ë¥¼ í™œìš©í•´ì„œ conditionì´ ì°¸ì¼ ê²½ìš° xë¥¼, ì•„ë‹Œ ê²½ìš° yë¡œ!\n",
    "df['ê¸°ìˆ ìŠ¤íƒ'] = df['ê¸°ìˆ ìŠ¤íƒ'].str.replace(\"'\", '\"')\n",
    "df['ê¸°ìˆ ìŠ¤íƒ'] = np.where((df['ê¸°ìˆ ìŠ¤íƒ'].isnull()) | (df['ê¸°ìˆ ìŠ¤íƒ'] == \"[]\"), \"\"\"[\"\"]\"\"\", df['ê¸°ìˆ ìŠ¤íƒ'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë§ˆê°ì¼ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë§ˆê°ì¼ ì „ì²˜ë¦¬ \n",
    "df['ë§ˆê°ì¼'] = df['ë§ˆê°ì¼'].str.replace('ìƒì‹œ', 'ìƒì‹œ ì±„ìš©')\n",
    "df['ë§ˆê°ì¼'] = df['ë§ˆê°ì¼'].str.replace('ì±„ìš© ì±„ìš©', 'ì±„ìš©')\n",
    "df['ë§ˆê°ì¼'] = df['ë§ˆê°ì¼'].fillna('ìƒì‹œ ì±„ìš©')\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ 2023-04-12\n",
    "def format_date(date_str):\n",
    "    if date_str.startswith('ìƒì‹œ ì±„ìš©'):\n",
    "        return date_str\n",
    "#     elif date_str.startswith('ìƒì‹œ'):\n",
    "#         return date_str\n",
    "    elif ':' in date_str:\n",
    "        date_obj = datetime.datetime.strptime(date_str, '%yë…„ %mì›” %dì¼ %H:%Mê¹Œì§€')\n",
    "        formatted_date = date_obj.strftime('%Y-%m-%d')\n",
    "        return formatted_date\n",
    "    else:\n",
    "        return date_str\n",
    "\n",
    "df['ë§ˆê°ì¼'] = df['ë§ˆê°ì¼'].apply(format_date)\n",
    "\n",
    "# # ë‚ ì§œ í˜•ì‹ 2023-04-12 00:00\n",
    "# import datetime\n",
    "\n",
    "# def format_date(date_str):\n",
    "#     if date_str.startswith('ìƒì‹œ ì±„ìš©'):\n",
    "#         return date_str\n",
    "#     elif ':' in date_str:\n",
    "#         date_obj = datetime.datetime.strptime(date_str, '%yë…„ %mì›” %dì¼ %H:%Mê¹Œì§€')\n",
    "#         formatted_date = date_obj.strftime('%Y-%m-%d %H:%M')\n",
    "#         return formatted_date\n",
    "#     else:\n",
    "#         date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "#         formatted_date = date_obj.strftime('%Y-%m-%d') + ' 24:00'\n",
    "#         return formatted_date\n",
    "\n",
    "# df['ë§ˆê°ì¼'] = df['ë§ˆê°ì¼'].apply(format_date)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê·¼ë¬´ì§€ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì£¼ì†Œ ', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ëŒ€í•œë¯¼êµ­ ', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸íŠ¹ë³„ì‹œ', 'ì„œìš¸')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ì‹œ', 'ì„œìš¸')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ê²½ê¸°ë„', 'ê²½ê¸°')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì œì£¼íŠ¹ë³„ìì¹˜ë„', 'ì œì£¼')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì œì£¼íŠ¹ë³„ìì¹˜ë„', 'ì œì£¼')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ìš¸ì‚°ê´‘ì—­ì‹œ', 'ìš¸ì‚°')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ê²½ìƒë¶ë„', 'ê²½ë¶')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ë¶€ì‚°ê´‘ì—­ì‹œ', 'ë¶€ì‚°')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì¸ì²œê´‘ì—­ì‹œ', 'ì¸ì²œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ëŒ€ì „ì‹œ', 'ëŒ€ì „')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ëŒ€ì „ê´‘ì—­ì‹œ', 'ëŒ€ì „')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ëŒ€êµ¬ê´‘ì—­ì‹œ', 'ëŒ€êµ¬')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ê²½ê¸°', 'ê²½ê¸°')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì „ë¼ë‚¨ë„', 'ì „ë‚¨')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì¸ì²œì‹œ', 'ì¸ì²œ')\n",
    "\n",
    "\n",
    "# ë”°ë¡œ ì—´ì–´ë³´ê³  ì´ìƒí•œ ê²ƒ ì ëŠ” ê³³\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('Level ', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì…ë‹ˆë‹¤', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì…ë‹ˆë‹¤', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ëŒ€êµ¬ ëŒ€ì „', 'ëŒ€ì „')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ 06237', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace(' ì„œìš¸', 'ì„œìš¸')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace(r'ì„œìš¸ \\(06159\\)\\s*', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace(r'ì„œìš¸ 13449\\)\\s*', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace(r'\\(07807\\)\\s*', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace(r'\\(08380\\)\\s*', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace(r'\\(06159\\)\\s*', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace(r'\\[ë³¸ì‚¬\\]\\s*', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('\\nÂ·', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace(r'^\\(\\d+\\)\\s*', '', regex=True)\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace(r'\\[.*?\\]', '', regex=True)\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì—°ë‚¨ë¡œ13ê¸¸9', 'ì„œìš¸ ë§ˆí¬êµ¬ ì—°ë‚¨ë¡œ13ê¸¸9', regex=True)\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ê´‘ì£¼ ê´‘ì£¼ê´‘ì—­ì‹œ', 'ê´‘ì£¼')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ë…¼í˜„ë¡œ', 'ì„œìš¸ ê°•ë‚¨êµ¬ ë…¼í˜„ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì™•ì‹­ë¦¬ë¡œ', 'ì„œìš¸ ì„±ë™êµ¬ ì™•ì‹­ë¦¬ë¡œ')\n",
    "# df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ëŒ€ì¹˜ë¡œ', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì„œì´ˆë™', 'ì„œìš¸ ì„œì´ˆêµ¬ ì„œì´ˆë™')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ë§ˆí¬ëŒ€ë¡œ', 'ì„œìš¸ ë§ˆí¬êµ¬ ë§ˆí¬ëŒ€ë¡œ')\n",
    "# df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ê°•ë‚¨ëŒ€ë¡œ', '')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ë°˜í¬ëŒ€ë¡œ', 'ì„œìš¸ ì„œì´ˆêµ¬ ë°˜í¬ëŒ€ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì›ê²© ê·¼ë¬´', 'ì›ê²© ê·¼ë¬´')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ê°œí¬ë¡œ', 'ì„œìš¸ ê°•ë‚¨êµ¬ ê°œí¬ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì‚¼ì„±ë¡œ', 'ì„œìš¸ ê°•ë‚¨êµ¬ ì‚¼ì„±ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì—¬ì˜ë„', 'ì„œìš¸ ì˜ë“±í¬êµ¬ ì—¬ì˜ë„ë™')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ì„œìš¸', 'ì„œìš¸')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì„ ë¦‰ë¡œ', 'ì„œìš¸ ê°•ë‚¨êµ¬ ì„ ë¦‰ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ í…Œí—¤ë€ë¡œ', 'ì„œìš¸ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì‚¼ì„±ë™', 'ì„œìš¸ ê°•ë‚¨êµ¬ ì‚¼ì„±ë™')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì–‘ì¬ë™', 'ì„œìš¸ ì„œì´ˆêµ¬ ì–‘ì¬ë™')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ê²½ê¸° ê¸ˆí† ë¡œ', 'ê²½ê¸° ì„±ë‚¨ì‹œ ìˆ˜ì •êµ¬ ê¸ˆí† ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ëŒ€ì „ ê°€ì •ë¶ë¡œ', 'ëŒ€ì „ ìœ ì„±êµ¬ ê°€ì •ë¶ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ê°•ë‚¨ëŒ€ë¡œ 156', 'ì„œìš¸ ì„œì´ˆêµ¬ ê°•ë‚¨ëŒ€ë¡œ 156')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ê²½ê¸° íŒêµ', 'ê²½ê¸° ì„±ë‚¨ì‹œ ë¶„ë‹¹êµ¬ íŒêµë™')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ëŒ€êµ¬ ëŒ€êµ¬ì‹œë‚´', 'ëŒ€êµ¬ ì¤‘êµ¬ ë™ì„±ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì–‘í™”ë¡œ', 'ì„œìš¸ ë§ˆí¬êµ¬ ì–‘í™”ë¡œ')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ë°œì‚°ì—­', 'ì„œìš¸ ê°•ì„œêµ¬ ë§ˆê³¡ë™')\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].str.replace('ì„œìš¸ ì–‘ì¬ì—­', 'ì„œìš¸ ì„œì´ˆêµ¬ ì–‘ì¬ì—­')\n",
    "# 'ê·¼ë¬´ì§€' ì»¬ëŸ¼ì˜ ë°ì´í„° í˜•ì‹ ë³€ê²½\n",
    "df.loc[df['ê·¼ë¬´ì§€'].str.contains('Seoul, Republic of Korea', na=False), 'ê·¼ë¬´ì§€'] = None\n",
    "df.loc[df['ê·¼ë¬´ì§€'].str.contains('ì„œìš¸ ëŒ€ì¹˜ë¡œ 223', na=False), 'ê·¼ë¬´ì§€'] = None\n",
    "df.loc[df['ê·¼ë¬´ì§€'].str.contains('ì›ê²© ê·¼ë¬´', na=False), 'ê·¼ë¬´ì§€'] = None\n",
    "df.loc[df['ê·¼ë¬´ì§€'].str.contains('onetkorea137@gmail.com', na=False), 'ê·¼ë¬´ì§€'] = None\n",
    "\n",
    "df.dropna(subset=['ê·¼ë¬´ì§€'], inplace=True)  # ê²°ì¸¡ì¹˜ ê°€ì§„ ë°ì´í„° ì‚­ì œ\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].astype(str)  # ë°ì´í„° íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "\n",
    "df['ê·¼ë¬´ì§€'] = df['ê·¼ë¬´ì§€'].apply(lambda x: x.split(' ', 1)[1] if len(x.split(' ')) > 1 and x.split(' ')[0] == x.split(' ')[1] else x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# íšŒì‚¬ í‰ì  ì»¬ëŸ¼ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xinics 3.8\n",
      "ìŠ¤ì™€ì¹˜ì˜¨ 3.3\n",
      "ë°”í… ë„¤íŠ¸ì›ìŠ¤ 1.6\n",
      "ì•Œë¡œì¹´ë„ìŠ¤ 0.0\n",
      "ìƒˆë¡ ì†”ë£¨ì…˜ 0.0\n",
      "ì—…ìŠ¤í…Œì´ì§€ 2.0\n",
      "ì›°íŠ¸ 3.4\n"
     ]
    }
   ],
   "source": [
    "# íšŒì‚¬ëª… ì¤‘ë³µê°’ì—†ì´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ê¸°\n",
    "company_list=df['íšŒì‚¬ëª…'].unique().tolist()\n",
    "\n",
    "# íšŒì‚¬ëª…ê³¼ í‰ì ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„° í”„ë ˆì„\n",
    "company_star_df = pd.DataFrame(columns=['íšŒì‚¬ëª…', 'í‰ì '])\n",
    "\n",
    "# í¬ë¡¤ë§ ì‹œì‘\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.jobplanet.co.kr/search?query=+&category=search_new&search_keyword_hint_id=&_rs_con=seach&_rs_act=keyword_search')\n",
    "time.sleep(3)  # 3ì´ˆ ë™ì•ˆ ëŒ€ê¸°\n",
    "\n",
    "for company in company_list:\n",
    "    # ê²€ìƒ‰ ë²„íŠ¼ ëˆ„ë¥´ê¸°\n",
    "    input_button = driver.find_element(By.XPATH, '//*[@id=\"search_bar_search_query\"]')\n",
    "    input_button.send_keys(company)\n",
    "    time.sleep(2)\n",
    "    star=0.0\n",
    "\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, 'ul > li.company')\n",
    "    if len(elements)==0:\n",
    "        pass\n",
    "    else:\n",
    "        submit_button=driver.find_element(By.CSS_SELECTOR, '#search_form > div > button')\n",
    "        submit_button.click()\n",
    "        time.sleep(2)  # 2ì´ˆ ë™ì•ˆ ëŒ€ê¸°\n",
    "        if len(elements)==1:\n",
    "            star=driver.find_element(By.XPATH,'//*[@id=\"mainContents\"]/div[1]/div/div[2]/div[1]/div/span[3]').text        \n",
    "        else:\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'result_card')\n",
    "            for card in cards:\n",
    "                jp_company = card.text.split()[0].replace(\"(ì£¼)\", \"\")\n",
    "                if company == jp_company:\n",
    "                    star = card.text.split()[2]\n",
    "                    break\n",
    "\n",
    "    print(company,star)    \n",
    "    company_star_df.loc[len(company_star_df)] = [company, star]\n",
    "    # ê²€ìƒ‰ì°½ ì§€ìš°ê¸°\n",
    "    input_button = driver.find_element(By.XPATH, '//*[@id=\"search_bar_search_query\"]')  # input_button ì›¹ ìš”ì†Œë¥¼ ë‹¤ì‹œ ì°¾ìŒ\n",
    "    input_button.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(company_star_df, on='íšŒì‚¬ëª…', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì§€ì—­ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ì§€ì—­']=df['ê·¼ë¬´ì§€'].apply(lambda address: \" \".join(address.split(\" \")[:2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„°í”„ë ˆì„ ê¹”ë”í•˜ê²Œ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['ê³µê³ ëª…','íšŒì‚¬ëª…','ì§ë¬´','ë§ˆê°ì¼','ê·¼ë¬´ì§€','ê¸°ìˆ ìŠ¤íƒ','ë§í¬','í‰ì ','ì§€ì—­']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Playdata\\Desktop\\final_true.csv', index=True, encoding='cp949')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
